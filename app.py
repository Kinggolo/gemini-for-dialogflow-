import os
import google.generativeai as genai
from flask import Flask, request, jsonify
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Initialize Gemini AI
genai.configure(api_key=GEMINI_API_KEY)

# Flask app setup
app = Flask(__name__)

# In-memory storage for user preferences and last question asked
user_preferences = {}
user_last_question = {}

# Root route for checking if the server is running
@app.route('/')
def home():
    return "It's working!"

# Custom prompt for Gemini AI
PROMPT_TEMPLATE = """
рддреБрдо рдПрдХ рдкрдврд╝рд╛рдИ рдореЗрдВ рдорджрдж рдХрд░рдиреЗ рд╡рд╛рд▓реЗ рд╕рд╣рд╛рдпрдХ рд╣реЛред рддреБрдореНрд╣рд╛рд░рд╛ рдХрд╛рдо рд╕рд┐рд░реНрдл study-related рд╕рд╡рд╛рд▓реЛрдВ рдХреЗ рдЬрд╡рд╛рдм рджреЗрдирд╛ рд╣реИред

ЁЯУМ **Speedy Current Affairs PDFs:**  
- рдЙрдкрд▓рдмреНрдз PDFs: January, February, рдФрд░ March (рд╣рд░ рдорд╣реАрдиреЗ рдХреА 5-10 рддрд╛рд░реАрдЦ рдХреЗ рдмреАрдЪ upload рд╣реЛрдЧреА)ред  
- рдЕрдЧрд░ рдХреЛрдИ рдЗрдирд╕реЗ related рдкреВрдЫреЗ, рддреЛ рдмрддрд╛рдУ рдХрд┐ рдЕрднреА рдХреМрди-рдХреМрди рд╕реА PDF рдЙрдкрд▓рдмреНрдз рд╣реИред  

ЁЯУМ **Study Time Table & Distraction рд╕реЗ рдмрдЪрд╛рд╡:**  
- рдЕрдЧрд░ рдХреЛрдИ Study Time Table рдпрд╛ рдкрдврд╝рд╛рдИ рдореЗрдВ рдзреНрдпрд╛рди рд▓рдЧрд╛рдиреЗ рдХреЗ рддрд░реАрдХреЗ рдкреВрдЫреЗ, рддреЛ рдЙрд╕реЗ рдПрдХ рдкреНрд░рднрд╛рд╡реА Time Table рдмрддрд╛рдУред  
- Focus рдмрдирд╛рдП рд░рдЦрдиреЗ рдХреЗ рддрд░реАрдХреЗ рдмрддрд╛рдУред  

ЁЯУМ **GK, GS рдФрд░ Science:**  
- рдЕрдЧрд░ рдХреЛрдИ GK, GS рдпрд╛ Science рд╕реЗ рдЬреБрдбрд╝рд╛ рд╕рд╡рд╛рд▓ рдкреВрдЫреЗ, рддреЛ рдЙрд╕реЗ рд╕рд╣реА рдЬрд╛рдирдХрд╛рд░реА рджреЛред  

ЁЯУМ **Books Availability:**  
- рдЕрдЧрд░ рдХреЛрдИ рдХрд┐рд╕реА рдХрд┐рддрд╛рдм рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдкреВрдЫреЗ, рддреЛ рдХрд╣реЛ: "рдЕрднреА рдпрд╣ рдЙрдкрд▓рдмреНрдз рдирд╣реАрдВ рд╣реИ, рд▓реЗрдХрд┐рди рднрд╡рд┐рд╖реНрдп рдореЗрдВ рдЗрд╕реЗ рднреА рдЕрдкрд▓реЛрдб рдХрд┐рдпрд╛ рдЬрд╛рдПрдЧрд╛ред"  

ЁЯУМ **Admin рд╕реЗ Contact:**  
- рдЕрдЧрд░ рдХреЛрдИ user admin рд╕реЗ рдмрд╛рдд рдХрд░рдирд╛ рдЪрд╛рд╣реЗ, рддреЛ professional рддрд░реАрдХреЗ рд╕реЗ рдХрд╣реЛ:  
  "рдЖрдк admin рд╕реЗ рдмрд╛рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП @aveshtrixbot рдкрд░ рд╕рдВрдкрд░реНрдХ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред"

тЪа **Important:**  
- Non-study topics рдкрд░ response рдордд рджреЛред  
- рдЕрдЧрд░ рдХреЛрдИ irrelevant рд╕рд╡рд╛рд▓ рдкреВрдЫреЗ, рддреЛ politely рдХрд╣реЛ:  
  "рдореИрдВ рд╕рд┐рд░реНрдл рдкрдврд╝рд╛рдИ рд╕реЗ рдЬреБрдбрд╝реА рдЬрд╛рдирдХрд╛рд░реА рджреЗ рд╕рдХрддрд╛ рд╣реВрдБред рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП рдЖрдк admin рд╕реЗ @aveshtrixbot рдкрд░ рд╕рдВрдкрд░реНрдХ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред"
  
ЁЯУМ **Quiz Interaction:**  
- **рд╣рд░ рдЬрд╡рд╛рдм рдХреЗ рдмрд╛рдж:** "рдХреНрдпрд╛ рдореИрдВ рдЖрдкрд╕реЗ рдПрдХ рд╕рд╡рд╛рд▓ рдкреВрдЫ рд╕рдХрддрд╛ рд╣реВрдБ?"  
- **рдЕрдЧрд░ user "рд╣рд╛рдБ" рдпрд╛ "yes" рдХрд╣реЗ:** рддреЛ рдПрдХ GK, GS, рдпрд╛ Current Affairs рдХрд╛ рд╕рд╡рд╛рд▓ рдкреВрдЫреЛред
- **рдЕрдЧрд░ рд╕рд╣реА рдЬрд╡рд╛рдм рджреЗ:** рддреЛ "Well done!" рдХрд╣реЛ рдФрд░ рдкреВрдЫреЛ "рдФрд░ рд╕рд╡рд╛рд▓ рдкреВрдЫреВрдВ рдпрд╛ рдХреБрдЫ рдФрд░ рдЬрд╛рдирдирд╛ рдЪрд╛рд╣рддреЗ рд╣реЛ?"
- **рдЕрдЧрд░ рдЧрд▓рдд рдЬрд╡рд╛рдм рджреЗ:** рддреЛ рд╕рд╣реА рдЬрд╡рд╛рдм рд╕рдВрдХреНрд╖реЗрдк рдореЗрдВ рдмрддрд╛рдУред  
"""

# Function to get Gemini AI response
def get_gemini_response(user_query):
    """ Gemini AI рд╕реЗ controlled response рд▓реЗрдиреЗ рдХреЗ рд▓рд┐рдП function """
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")  # Flash model use рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ
        response = model.generate_content(PROMPT_TEMPLATE + f"\nUser: {user_query}\nAssistant:")
        return response.text if response.text else "рдореБрдЭреЗ рд╕рдордЭ рдирд╣реАрдВ рдЖрдпрд╛, рдХреГрдкрдпрд╛ рдлрд┐рд░ рд╕реЗ рдкреВрдЫреЗрдВред"
    except Exception as e:
        print(f"Error: {e}")
        return "рдЕрднреА рддрдХрдиреАрдХреА рд╕рдорд╕реНрдпрд╛ рд╣реИ, рдХреГрдкрдпрд╛ рдмрд╛рдж рдореЗрдВ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред"

# Function to generate a new quiz question dynamically
def generate_quiz_question():
    """ Gemini AI рд╕реЗ рдирдпрд╛ random GK, GS, рдпрд╛ Current Affairs рд╕рд╡рд╛рд▓ generate рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП function """
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content("рдХреГрдкрдпрд╛ рдПрдХ рдирдпрд╛, рд░реЛрдЪрдХ рдФрд░ informative GK, GS, рдпрд╛ Current Affairs рдХрд╛ рд╕рд╡рд╛рд▓ рдкреВрдЫреЗрдВред")
        return response.text.strip() if response.text else "рднрд╛рд░рдд рдХрд╛ рд░рд╛рд╖реНрдЯреНрд░реАрдп рдЦреЗрд▓ рдХреНрдпрд╛ рд╣реИ?"
    except Exception as e:
        print(f"Error generating question: {e}")
        return "рднрд╛рд░рдд рдХрд╛ рд░рд╛рд╖реНрдЯреНрд░реАрдп рдЦреЗрд▓ рдХреНрдпрд╛ рд╣реИ?"  # Default fallback question

@app.route('/webhook', methods=['POST'])
def dialogflow_webhook():
    req_data = request.get_json()
    
    # Extract user query and user ID
    user_query = req_data.get("queryResult", {}).get("queryText", "").strip().lower()
    user_id = req_data.get("originalDetectIntentRequest", {}).get("payload", {}).get("user", {}).get("userId", "default_user")
    
    # Get last question asked
    last_question = user_last_question.get(user_id, None)

    response_messages = []  # Store multiple responses

    if last_question:
        # Validate the answer using Gemini AI
        validation_prompt = f"рдХреНрдпрд╛ рдпрд╣ рдЙрддреНрддрд░ '{user_query}' рдЗрд╕ рд╕рд╡рд╛рд▓ рдХрд╛ рд╕рд╣реА рдЬрд╡рд╛рдм рд╣реИ: '{last_question}'? рдЕрдЧрд░ рд╣рд╛рдБ, рддреЛ 'рд╕рд╣реА' рд▓рд┐рдЦреЛ, рдирд╣реАрдВ рддреЛ 'рдЧрд▓рдд' рд▓рд┐рдЦреЛ рдФрд░ рд╕рд╣реА рдЙрддреНрддрд░ рдмрддрд╛рдУред"
        validation_response = get_gemini_response(validation_prompt)

        if "рд╕рд╣реА" in validation_response:
            response_messages.append({"text": "рд╕рд╣реА рдЬрд╡рд╛рдм! ЁЯОЙ рдЕрдЪреНрдЫрд╛ рдХрд┐рдпрд╛!"})
        else:
            correct_answer = validation_response.replace("рдЧрд▓рдд", "").strip()
            response_messages.append({"text": f"рдЧрд▓рдд рдЬрд╡рд╛рдм! рд╕рд╣реА рдЙрддреНрддрд░ рд╣реИ: {correct_answer}ред"})

        # Generate and send a new quiz question separately
        new_question = generate_quiz_question()
        response_messages.append({"text": new_question})

        # Store the new question
        user_last_question[user_id] = new_question
    else:
        # Normal study-related query
        response_text = get_gemini_response(user_query)
        response_messages.append({"text": response_text})

        # Ask if the user wants a quiz question separately
        response_messages.append({"text": "рдХреНрдпрд╛ рдореИрдВ рдЖрдкрд╕реЗ рдПрдХ рд╕рд╡рд╛рд▓ рдкреВрдЫ рд╕рдХрддрд╛ рд╣реВрдБ?"})

        # Store a new dynamic quiz question for continuity
        user_last_question[user_id] = generate_quiz_question()

    # Return multiple responses
    return jsonify({"fulfillmentMessages": [{"text": msg} for msg in response_messages]})

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000)
